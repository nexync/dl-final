{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baldu\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n",
      "c:\\Users\\Baldu\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from minigrid.core.constants import COLOR_NAMES\n",
    "from minigrid.core.grid import Grid\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Door, Goal, Key, Wall\n",
    "from minigrid.manual_control import ManualControl\n",
    "from minigrid.minigrid_env import MiniGridEnv\n",
    "from minigrid.core.actions import Actions\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import minigrid\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "from gymnasium.core import ObservationWrapper\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "from plot import make_plot\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg') \n",
    "import matplotlib.pyplot as plt\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myenv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=5, \n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps: int | None = None,\n",
    "        **kwargs,):\n",
    "        \n",
    "        self.grid_size = size\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        \n",
    "        self.is_sb3_mode = False\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            see_through_walls=False,  # Set to False for a more realistic environment\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.door_opened = False\n",
    "        self.key_found = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"Find the key to open the door and reach the goal\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        # Create an empty grid\n",
    "        self.grid = Grid(width, height)\n",
    "\n",
    "        # Generate the surrounding walls\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Randomly place the door\n",
    "        door_color = COLOR_NAMES[0]\n",
    "        door_pos = self._random_position(exclude=[(1, 1)])  # Exclude agent's start position\n",
    "        self.grid.set(*door_pos, Door(door_color, is_locked=True))\n",
    "\n",
    "        # Randomly place the key\n",
    "        key_pos = self._random_position(exclude=[(1, 1), door_pos])  # Exclude agent's start position and door position\n",
    "        self.grid.set(*key_pos, Key(door_color))\n",
    "\n",
    "        # Place a goal square in the bottom-right corner\n",
    "        self.put_obj(Goal(), width - 2, height - 2)\n",
    "\n",
    "        # Place the agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"Find the key to open the door and reach the goal\"\n",
    "    \n",
    "    def _random_position(self, exclude=[]):\n",
    "        while True:\n",
    "            pos = (random.randint(1, self.grid_size - 2), random.randint(1, self.grid_size - 2))\n",
    "            if pos not in exclude and self.grid.get(*pos) is None:\n",
    "                return pos\n",
    "            \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Reinitialize episode-specific variables\n",
    "        self.agent_pos = (-1, -1)\n",
    "        self.agent_dir = -1\n",
    "\n",
    "        # Generate a new random grid at the start of each episode\n",
    "        self._gen_grid(self.width, self.height)\n",
    "\n",
    "        # These fields should be defined by _gen_grid\n",
    "        assert (\n",
    "            self.agent_pos >= (0, 0)\n",
    "            if isinstance(self.agent_pos, tuple)\n",
    "            else all(self.agent_pos >= 0) and self.agent_dir >= 0\n",
    "        )\n",
    "\n",
    "        # Check that the agent doesn't overlap with an object\n",
    "        start_cell = self.grid.get(*self.agent_pos)\n",
    "        assert start_cell is None or start_cell.can_overlap()\n",
    "\n",
    "        # Item picked up, being carried, initially nothing\n",
    "        self.carrying = None\n",
    "        self.key_found = False\n",
    "        self.door_opened = False\n",
    "\n",
    "        # Step count since episode start\n",
    "        self.step_count = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        # Return first observation\n",
    "        obs = self.gen_obs()\n",
    "\n",
    "        return obs, {}\n",
    "            \n",
    "            \n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Get the position in front of the agent\n",
    "        fwd_pos = self.front_pos\n",
    "\n",
    "        # Get the contents of the cell in front of the agent\n",
    "        fwd_cell = self.grid.get(*fwd_pos)\n",
    "\n",
    "        # Check for actions and update state\n",
    "        if action == self.actions.pickup:\n",
    "            if fwd_cell and fwd_cell.can_pickup() and self.carrying is None:\n",
    "                self.carrying = fwd_cell\n",
    "                self.grid.set(*fwd_pos, None)\n",
    "                if isinstance(fwd_cell, Key):\n",
    "                    self.key_found = True  # Flag for key pickup\n",
    "                    reward += 0.1  # Reward for picking up the key\n",
    "\n",
    "        elif action == self.actions.toggle:\n",
    "            if fwd_cell and isinstance(fwd_cell, Door) and self.carrying and isinstance(self.carrying, Key):\n",
    "                fwd_cell.toggle(self, fwd_pos)\n",
    "                self.door_opened = True  # Flag for door opening\n",
    "                reward += 0.1  # Reward for opening the door\n",
    "\n",
    "        # Rotate left\n",
    "        if action == self.actions.left:\n",
    "            self.agent_dir -= 1\n",
    "            if self.agent_dir < 0:\n",
    "                self.agent_dir += 4\n",
    "\n",
    "        # Rotate right\n",
    "        elif action == self.actions.right:\n",
    "            self.agent_dir = (self.agent_dir + 1) % 4\n",
    "\n",
    "        # Move forward\n",
    "        elif action == self.actions.forward:\n",
    "            if fwd_cell is None or fwd_cell.can_overlap():\n",
    "                self.agent_pos = tuple(fwd_pos)\n",
    "            if fwd_cell is not None and fwd_cell.type == \"goal\":\n",
    "                if self.key_found and self.door_opened:\n",
    "                    terminated = True\n",
    "                    reward += self._reward()  # Reward for completing all tasks\n",
    "                else:\n",
    "                    terminated = True\n",
    "                    reward -= 0.1\n",
    "            if fwd_cell is not None and fwd_cell.type == \"lava\":\n",
    "                terminated = True\n",
    "                \n",
    "\n",
    "        # Check for max steps\n",
    "        if self.step_count >= self.max_steps:\n",
    "            truncated = True\n",
    "\n",
    "        # Render if in human mode\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        obs = self.gen_obs()\n",
    "\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     env = myenv(render_mode=\"human\")\n",
    "\n",
    "#     # Enable manual control for testing\n",
    "#     manual_control = ManualControl(env, seed=42)\n",
    "#     manual_control.start()\n",
    "\n",
    "    # obs = env.reset()\n",
    "    # for _ in range(10):  # Just run a few steps for testing\n",
    "    #     action = env.action_space.sample()  # Random action\n",
    "    #     obs, reward, done, info = env.step(action)\n",
    "    #     env.render()  # Make sure to render the environment\n",
    "    #     if done:\n",
    "    #         break\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnv(MiniGridEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=7,\n",
    "        agent_start_pos=(1, 1),\n",
    "        agent_start_dir=0,\n",
    "        max_steps: int | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.agent_start_pos = agent_start_pos\n",
    "        self.agent_start_dir = agent_start_dir\n",
    "        self.grid_size = size\n",
    "        \n",
    "        mission_space = MissionSpace(mission_func=self._gen_mission)\n",
    "\n",
    "        if max_steps is None:\n",
    "            max_steps = 4 * size**2\n",
    "\n",
    "        super().__init__(\n",
    "            mission_space=mission_space,\n",
    "            grid_size=size,\n",
    "            see_through_walls=False,  # Set to True for maximum speed\n",
    "            max_steps=max_steps,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_mission():\n",
    "        return \"grand mission\"\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        self.grid = Grid(width, height)\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Dynamic Wall Placement\n",
    "        wall_col = width - 4  # Adjusted to leave more space on the right\n",
    "        for i in range(height):\n",
    "            self.grid.set(wall_col, i, Wall())\n",
    "\n",
    "        # Randomly place the door in the same column as the wall\n",
    "        door_pos = (wall_col, random.randint(1, height - 2))\n",
    "        self.grid.set(*door_pos, Door(COLOR_NAMES[0], is_locked=True))\n",
    "\n",
    "        # Define the region for the key (left side of the wall)\n",
    "        key_region_end_x = wall_col - 1\n",
    "\n",
    "        # Randomly place the key on the left side of the wall\n",
    "        key_pos = self._random_position(\n",
    "            exclude=[self.agent_start_pos, door_pos] + [(wall_col, i) for i in range(height)],\n",
    "            max_x=key_region_end_x\n",
    "        )\n",
    "        self.grid.set(*key_pos, Key(COLOR_NAMES[0]))\n",
    "\n",
    "        # Place the goal in a 3x3 region in the bottom right corner\n",
    "        goal_region_start_x = width - 3\n",
    "        goal_region_start_y = height - 3\n",
    "        goal_pos = (random.randint(goal_region_start_x, width - 2),\n",
    "                    random.randint(goal_region_start_y, height - 2))\n",
    "        self.put_obj(Goal(), *goal_pos)\n",
    "\n",
    "        # Place the agent\n",
    "        if self.agent_start_pos is not None:\n",
    "            self.agent_pos = self.agent_start_pos\n",
    "            self.agent_dir = self.agent_start_dir\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        self.mission = \"grand mission\"\n",
    "\n",
    "    def _random_position(self, exclude=[], max_x=None):\n",
    "        while True:\n",
    "            x_range = self.grid_size - 2 if max_x is None else max_x\n",
    "            pos = (random.randint(1, x_range), random.randint(1, self.grid_size - 2))\n",
    "            if pos not in exclude and self.grid.get(*pos) is None:\n",
    "                return pos\n",
    "            \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Reinitialize episode-specific variables\n",
    "        self.agent_pos = (-1, -1)\n",
    "        self.agent_dir = -1\n",
    "\n",
    "        # Generate a new random grid at the start of each episode\n",
    "        self._gen_grid(self.width, self.height)\n",
    "\n",
    "        # These fields should be defined by _gen_grid\n",
    "        assert (\n",
    "            self.agent_pos >= (0, 0)\n",
    "            if isinstance(self.agent_pos, tuple)\n",
    "            else all(self.agent_pos >= 0) and self.agent_dir >= 0\n",
    "        )\n",
    "\n",
    "        # Check that the agent doesn't overlap with an object\n",
    "        start_cell = self.grid.get(*self.agent_pos)\n",
    "        assert start_cell is None or start_cell.can_overlap()\n",
    "\n",
    "        # Item picked up, being carried, initially nothing\n",
    "        self.carrying = None\n",
    "        self.key_found = False\n",
    "        self.door_opened = False\n",
    "\n",
    "        # Step count since episode start\n",
    "        self.step_count = 0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        # Return first observation\n",
    "        obs = self.gen_obs()\n",
    "\n",
    "        return obs, {}\n",
    "            \n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "\n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # Get the position in front of the agent\n",
    "        fwd_pos = self.front_pos\n",
    "\n",
    "        # Get the contents of the cell in front of the agent\n",
    "        fwd_cell = self.grid.get(*fwd_pos)\n",
    "\n",
    "        # Check for actions and update state\n",
    "        if action == self.actions.pickup:\n",
    "            if fwd_cell and fwd_cell.can_pickup() and self.carrying is None:\n",
    "                self.carrying = fwd_cell\n",
    "                self.grid.set(*fwd_pos, None)\n",
    "                if isinstance(fwd_cell, Key):\n",
    "                    self.key_found = True  # Flag for key pickup\n",
    "                    reward += 0.1  # Reward for picking up the key\n",
    "\n",
    "        elif action == self.actions.toggle:\n",
    "            if fwd_cell and isinstance(fwd_cell, Door) and self.carrying and isinstance(self.carrying, Key):\n",
    "                # Toggle the door only if it's not already opened\n",
    "                if not fwd_cell.is_open:\n",
    "                    fwd_cell.toggle(self, fwd_pos)\n",
    "                    if not self.door_opened:  # Check if this is the first time opening the door\n",
    "                        self.door_opened = True\n",
    "                        reward += 0.1 \n",
    "\n",
    "        # Rotate left\n",
    "        if action == self.actions.left:\n",
    "            self.agent_dir -= 1\n",
    "            if self.agent_dir < 0:\n",
    "                self.agent_dir += 4\n",
    "\n",
    "        # Rotate right\n",
    "        elif action == self.actions.right:\n",
    "            self.agent_dir = (self.agent_dir + 1) % 4\n",
    "\n",
    "        # Move forward\n",
    "        elif action == self.actions.forward:\n",
    "            if fwd_cell is None or fwd_cell.can_overlap():\n",
    "                self.agent_pos = tuple(fwd_pos)\n",
    "            if fwd_cell is not None and fwd_cell.type == \"goal\":\n",
    "                if self.key_found and self.door_opened:\n",
    "                    terminated = True\n",
    "                    reward += self._reward()  # Reward for completing all tasks\n",
    "                else:\n",
    "                    terminated = True\n",
    "                    reward -= 0.1\n",
    "            if fwd_cell is not None and fwd_cell.type == \"lava\":\n",
    "                terminated = True\n",
    "                \n",
    "\n",
    "        # Check for max steps\n",
    "        if self.step_count >= self.max_steps:\n",
    "            truncated = True\n",
    "\n",
    "        # Render if in human mode\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        obs = self.gen_obs()\n",
    "\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "            \n",
    "\n",
    "\n",
    "# def main():\n",
    "#     env = SimpleEnv(render_mode=\"human\")\n",
    "\n",
    "#     # enable manual control for testing\n",
    "#     manual_control = ManualControl(env, seed=42)\n",
    "#     manual_control.start()\n",
    "\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = myenv(render_mode=\"human\")\n",
    "# manual_control = ManualControl(env)\n",
    "# manual_control.start()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 512):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        flattened_size = 64\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(flattened_size, features_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2) \n",
    "        )\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MinigridFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgObsWrapper2(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = env.observation_space.spaces[\"image\"]\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return obs[\"image\"]\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self.observation(obs), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, reward_threshold, verbose=1):\n",
    "        super(CustomRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.total_rewards = 0\n",
    "        self.episode_rewards = []\n",
    "        self.mean_rewards = []\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.total_rewards += self.locals['rewards'][0]\n",
    "\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_rewards.append(self.total_rewards)\n",
    "            current_mean_reward = np.mean(self.episode_rewards[-100:])  \n",
    "            self.mean_rewards.append(current_mean_reward)\n",
    "            self.total_rewards = 0\n",
    "\n",
    "            make_plot(self.episode_rewards, self.mean_rewards, self.fig, self.ax)\n",
    "\n",
    "            if current_mean_reward >= self.reward_threshold:\n",
    "                print(f\"Stopping training as the mean reward {current_mean_reward} is above the threshold {self.reward_threshold}\")\n",
    "                return False  # Return False to stop the training\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# Setup model and environment\n",
    "env = SimpleEnv(render_mode=\"human\")\n",
    "env = ImgObsWrapper(env)\n",
    "model = PPO(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "\n",
    "# Instantiate the callback\n",
    "max_reward = 0.85\n",
    "callback = CustomRewardCallback(check_freq=1000, reward_threshold=max_reward)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=int(2e5), callback=callback)\n",
    "model.save(\"PPO_model\")\n",
    "\n",
    "# Optionally save the final plot\n",
    "plt.savefig('final_training_plot.png')\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 311      |\n",
      "|    ep_rew_mean     | 0.383    |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 318        |\n",
      "|    ep_rew_mean          | 0.359      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 418        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03623655 |\n",
      "|    clip_fraction        | 0.408      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | -0.751     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0783     |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | 0.023      |\n",
      "|    value_loss           | 0.00512    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 327         |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 627         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031246025 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    value_loss           | 0.00243     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 325         |\n",
      "|    ep_rew_mean          | 0.368       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023084499 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00265    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    value_loss           | 0.00171     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 338         |\n",
      "|    ep_rew_mean          | 0.328       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025238948 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00343    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00632    |\n",
      "|    value_loss           | 0.00182     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 341         |\n",
      "|    ep_rew_mean          | 0.311       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1256        |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025094517 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.165      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00353    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    value_loss           | 0.000994    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 330         |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1466        |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020587638 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00963    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00432    |\n",
      "|    value_loss           | 0.00171     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 326         |\n",
      "|    ep_rew_mean          | 0.347       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1676        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026822992 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00334    |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 321         |\n",
      "|    ep_rew_mean          | 0.364       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1885        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028325845 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00853    |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 298         |\n",
      "|    ep_rew_mean          | 0.429       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 2097        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035442658 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00521    |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 282         |\n",
      "|    ep_rew_mean          | 0.478       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 2308        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031812206 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00435    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00376    |\n",
      "|    value_loss           | 0.00857     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 272         |\n",
      "|    ep_rew_mean          | 0.504       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 2519        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030613283 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0023     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00172     |\n",
      "|    value_loss           | 0.00915     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 253         |\n",
      "|    ep_rew_mean          | 0.556       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 2731        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027480565 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.514       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    value_loss           | 0.00877     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 223         |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 2943        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029032832 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00441     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00668    |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 189         |\n",
      "|    ep_rew_mean          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 3155        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041011482 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0358     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.00339     |\n",
      "|    value_loss           | 0.00875     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 183        |\n",
      "|    ep_rew_mean          | 0.751      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 3365       |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04362724 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.548      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00899   |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | 0.00309    |\n",
      "|    value_loss           | 0.00782    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 156         |\n",
      "|    ep_rew_mean          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 3578        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029378453 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00958    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.00031     |\n",
      "|    value_loss           | 0.00557     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 157        |\n",
      "|    ep_rew_mean          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 3789       |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03600885 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.571      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.00449   |\n",
      "|    value_loss           | 0.00871    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 140         |\n",
      "|    ep_rew_mean          | 0.872       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 4002        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031712808 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.000546   |\n",
      "|    value_loss           | 0.00994     |\n",
      "-----------------------------------------\n",
      "Stopping training as the mean reward 0.9006150032579899 is above the threshold 0.9\n"
     ]
    }
   ],
   "source": [
    "env = SimpleEnv(size=10, render_mode = 'human')\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "# Load the trained model, ensure to provide the correct path\n",
    "model = PPO.load(\"PPO_model\", env=env)\n",
    "\n",
    "max_reward = 0.9\n",
    "callback = CustomRewardCallback(check_freq=1000, reward_threshold=max_reward)\n",
    "model.learn(total_timesteps=int(2e5), callback=callback)\n",
    "model.save(\"PPO_model_8x8\")\n",
    "plt.savefig('training_plot_8x8.png')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Stopping training as the mean reward 1.0906250029802322 is above the threshold 0.9\n"
     ]
    }
   ],
   "source": [
    "env = SimpleEnv(size=12, render_mode = 'human')\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "# Load the trained model, ensure to provide the correct path\n",
    "model = PPO.load(\"PPO_model_8x8\", env=env)\n",
    "\n",
    "max_reward = 0.9\n",
    "callback = CustomRewardCallback(check_freq=1000, reward_threshold=max_reward)\n",
    "model.learn(total_timesteps=int(2e5), callback=callback)\n",
    "model.save(\"PPO_model_10x10\")\n",
    "plt.savefig('training_plot_10x10.png')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 235      |\n",
      "|    ep_rew_mean     | 0.993    |\n",
      "| time/              |          |\n",
      "|    fps             | 9        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 376         |\n",
      "|    ep_rew_mean          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 416         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040102206 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.00658     |\n",
      "|    value_loss           | 0.00533     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 412         |\n",
      "|    ep_rew_mean          | 0.807       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 625         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027348392 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -0.0702     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | 0.0074      |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 473         |\n",
      "|    ep_rew_mean          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 834         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029427305 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00333    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00357    |\n",
      "|    value_loss           | 0.00246     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 535         |\n",
      "|    ep_rew_mean          | 0.663       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1043        |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035746507 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0256     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.00493     |\n",
      "|    value_loss           | 0.00155     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 558         |\n",
      "|    ep_rew_mean          | 0.637       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1252        |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055523787 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.191      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00858    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 0.000737    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 559        |\n",
      "|    ep_rew_mean          | 0.636      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 1461       |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07230019 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | 0.305      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0465    |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | 0.00915    |\n",
      "|    value_loss           | 0.00209    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 578         |\n",
      "|    ep_rew_mean          | 0.618       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1670        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030834714 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00923    |\n",
      "|    value_loss           | 0.00306     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 581         |\n",
      "|    ep_rew_mean          | 0.616       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1880        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035699487 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00879     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.000923    |\n",
      "|    value_loss           | 0.00164     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 600         |\n",
      "|    ep_rew_mean          | 0.602       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 2089        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034509927 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.000601    |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 575         |\n",
      "|    ep_rew_mean          | 0.628       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 2299        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026493665 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00296    |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 558         |\n",
      "|    ep_rew_mean          | 0.646       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 2509        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028749652 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | 0.000702    |\n",
      "|    value_loss           | 0.00452     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 519        |\n",
      "|    ep_rew_mean          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 2719       |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02546101 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.621      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0227     |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00258   |\n",
      "|    value_loss           | 0.0041     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 517        |\n",
      "|    ep_rew_mean          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 2928       |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02822664 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.569      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0124    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | 0.000888   |\n",
      "|    value_loss           | 0.00957    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 474         |\n",
      "|    ep_rew_mean          | 0.735       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 3139        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019849543 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00502    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 0.00243     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 475         |\n",
      "|    ep_rew_mean          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 3348        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034337725 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00209    |\n",
      "|    value_loss           | 0.0103      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 464         |\n",
      "|    ep_rew_mean          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 3557        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027453916 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0213      |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    value_loss           | 0.00592     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 451         |\n",
      "|    ep_rew_mean          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 3767        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021453658 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00166    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 0.00755     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 437         |\n",
      "|    ep_rew_mean          | 0.773       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 3977        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032637328 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | 0.000335    |\n",
      "|    value_loss           | 0.00855     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 434         |\n",
      "|    ep_rew_mean          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 4186        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028499357 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00141    |\n",
      "|    value_loss           | 0.00939     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 427        |\n",
      "|    ep_rew_mean          | 0.787      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 4396       |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03157037 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.469      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0054     |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00726   |\n",
      "|    value_loss           | 0.00742    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 434         |\n",
      "|    ep_rew_mean          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 4606        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027657159 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00842    |\n",
      "|    value_loss           | 0.00732     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 397        |\n",
      "|    ep_rew_mean          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 4816       |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03524514 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.668      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00825   |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.00629   |\n",
      "|    value_loss           | 0.0067     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 364         |\n",
      "|    ep_rew_mean          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 5026        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027657036 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00688    |\n",
      "|    value_loss           | 0.00897     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 337        |\n",
      "|    ep_rew_mean          | 0.887      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 5236       |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02418456 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.927     |\n",
      "|    explained_variance   | 0.632      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00414    |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.00621   |\n",
      "|    value_loss           | 0.00837    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 328        |\n",
      "|    ep_rew_mean          | 0.894      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 5445       |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03679014 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.924     |\n",
      "|    explained_variance   | 0.53       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0227    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | 0.000335   |\n",
      "|    value_loss           | 0.00811    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 338         |\n",
      "|    ep_rew_mean          | 0.885       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 5654        |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046833754 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.787      |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0793      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    value_loss           | 0.00656     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 329         |\n",
      "|    ep_rew_mean          | 0.894       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 5865        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031012494 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.794      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0399      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    value_loss           | 0.00375     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 334         |\n",
      "|    ep_rew_mean          | 0.888       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 6074        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032571495 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.814      |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00306     |\n",
      "|    value_loss           | 0.00529     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 335        |\n",
      "|    ep_rew_mean          | 0.887      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 6284       |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02064035 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.746     |\n",
      "|    explained_variance   | 0.629      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.00395   |\n",
      "|    value_loss           | 0.00479    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 339         |\n",
      "|    ep_rew_mean          | 0.884       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 6494        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030903658 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.719      |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 0.00589     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 332        |\n",
      "|    ep_rew_mean          | 0.893      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 6704       |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06755494 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.818     |\n",
      "|    explained_variance   | 0.656      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0184    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0048    |\n",
      "|    value_loss           | 0.00629    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 323        |\n",
      "|    ep_rew_mean          | 0.903      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 6914       |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03556495 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.735     |\n",
      "|    explained_variance   | 0.61       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.019      |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.003     |\n",
      "|    value_loss           | 0.00849    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 301         |\n",
      "|    ep_rew_mean          | 0.924       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 7126        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044529717 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.743      |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 0.00835     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 293        |\n",
      "|    ep_rew_mean          | 0.931      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 7337       |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06723368 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.779     |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 650        |\n",
      "|    policy_gradient_loss | 0.0218     |\n",
      "|    value_loss           | 0.0127     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 283        |\n",
      "|    ep_rew_mean          | 0.941      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9          |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 7547       |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06312692 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.787     |\n",
      "|    explained_variance   | 0.638      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00547    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.000973  |\n",
      "|    value_loss           | 0.00998    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 278         |\n",
      "|    ep_rew_mean          | 0.944       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 7757        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030184194 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.752      |\n",
      "|    explained_variance   | 0.71        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00227    |\n",
      "|    value_loss           | 0.00901     |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 255       |\n",
      "|    ep_rew_mean          | 0.968     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 9         |\n",
      "|    iterations           | 38        |\n",
      "|    time_elapsed         | 7970      |\n",
      "|    total_timesteps      | 77824     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0398055 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.655    |\n",
      "|    explained_variance   | 0.539     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.028    |\n",
      "|    n_updates            | 680       |\n",
      "|    policy_gradient_loss | -0.00765  |\n",
      "|    value_loss           | 0.00722   |\n",
      "---------------------------------------\n",
      "Stopping training as the mean reward 1.0013574235141278 is above the threshold 1.0\n"
     ]
    }
   ],
   "source": [
    "env = SimpleEnv(size=16, render_mode = 'human')\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "# Load the trained model, ensure to provide the correct path\n",
    "model = PPO.load(\"PPO_model_10x10\", env=env)\n",
    "\n",
    "max_reward = 1.0\n",
    "callback = CustomRewardCallback(check_freq=1000, reward_threshold=max_reward)\n",
    "model.learn(total_timesteps=int(2e5), callback=callback)\n",
    "model.save(\"PPO_model_16x16\")\n",
    "plt.savefig('training_plot_16x16.png')\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "443827bf24667f421fb10725def0166fa1673cc655d9aeee845987250124cade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
